Video Title: Watching Neural Networks Learn
URL: https://www.youtube.com/watch?v=TkwXa7Cvfr8
Duration: 25:27
Upload Date: 20230817

Summary: - Neural networks are universal function approximators and can learn and predict the world around us.
- Functions describe the world and neural networks can build their own functions.
- Neural networks can approximate functions based on a sample of data points through curve fitting.
- The structure of a neural network includes neurons, weights, and activation functions.
- The training process aims to minimize the network's error or loss.
- Higher dimensional problems pose challenges for neural networks.
- Techniques such as normalizing inputs and using different activation functions can improve function approximation.
- Testing and validating ideas through empirical evidence is important.
- Other methods for approximating functions include Taylor series and Fourier series.
- Neural networks are effective for low-dimensional problems but struggle with higher-dimensional problems.
- Taylor series approximates functions around a specific point, while Fourier series approximates functions within a given range of points.
- Using Fourier features in a neural network can improve function approximation, particularly in image datasets.
- Fourier features are not commonly used in real-world neural networks.
- The MNIST dataset is a real-world problem with high-dimensional inputs.
- The curse of dimensionality is a challenge in high-dimensional problems.
- Comparing a normal neural network and a Fourier network on the MNIST dataset shows that the Fourier network is not as effective for high-dimensional problems.
- Different problems require different approaches in function approximation.
- Further exploration of the potential of Fourier series in machine learning is encouraged.
- The Mandelbrot approximation problem is presented as a fun challenge for those interested in exploring function approximation further.

Video Description: A video about neural networks, function approximation, machine learning, and mathematical building blocks. Dennis Nedry did nothing wrong. This is a submission for #SoME3

My Links
Patreon: https://www.patreon.com/emergentgarden
Discord: https://discord.gg/ZsrAAByEnr

Links and Content:
On Mathematical Maturity, Thomas Garrity: https://www.youtube.com/watch?v=zHU1xH6Ogs4
Earth Rotation Loop: https://www.youtube.com/watch?v=aiQdLP2mBJE
Modeling Shell Surfaces: https://www.geogebra.org/m/xtv7zpn5
Fourier Features Paper: https://arxiv.org/abs/2006.10739
Code for mandelbrot/image approximations: https://github.com/MaxRobinsonTheGreat/mandelbrotnn
Code for line/surface approximations: https://github.com/MaxRobinsonTheGreat/ManimApproximations

Music: https://youtube.com/@acolyte-compositions

Timestamps
(0:00) Functions Describe the World
(3:15) Neural Architecture
(5:35) Higher Dimensions
(11:55) Taylor Series
(15:20) Fourier Series
(21:25) The Real World
(24:32) An Open Challenge

